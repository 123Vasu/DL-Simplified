{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Reshape, Bidirectional, LSTM, Dense, Lambda, Activation, BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training and testing data\n",
    "train = pd.read_csv('F:\\Yash\\Dataset_SSOC\\written_name_train_v2.csv')\n",
    "valid = pd.read_csv('F:\\Yash\\Dataset_SSOC\\written_name_validation_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Image data of dtype object cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\Yash\\SSOC\\project_1\\DL-Simplified\\Handwriting Recognition Project\\Model\\handwriting_recognition_project.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#X63sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m img_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mF:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mYash\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mDataset_SSOC\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mrain_v2\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mrain\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mtrain\u001b[39m.\u001b[39mloc[i, \u001b[39m'\u001b[39m\u001b[39mFILENAME\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#X63sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(img_dir, cv2\u001b[39m.\u001b[39mIMREAD_GRAYSCALE)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#X63sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m plt\u001b[39m.\u001b[39;49mimshow(image, cmap \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mgray\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#X63sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(train\u001b[39m.\u001b[39mloc[i, \u001b[39m'\u001b[39m\u001b[39mIDENTITY\u001b[39m\u001b[39m'\u001b[39m], fontsize\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#X63sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m plt\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39moff\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\matplotlib\\_api\\deprecation.py:456\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m name_idx:\n\u001b[0;32m    451\u001b[0m     warn_deprecated(\n\u001b[0;32m    452\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    455\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\matplotlib\\pyplot.py:2640\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mimshow)\n\u001b[0;32m   2635\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimshow\u001b[39m(\n\u001b[0;32m   2636\u001b[0m         X, cmap\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, norm\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, aspect\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, interpolation\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2637\u001b[0m         alpha\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, origin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, extent\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m,\n\u001b[0;32m   2638\u001b[0m         interpolation_stage\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, filternorm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, filterrad\u001b[39m=\u001b[39m\u001b[39m4.0\u001b[39m,\n\u001b[0;32m   2639\u001b[0m         resample\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, url\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 2640\u001b[0m     __ret \u001b[39m=\u001b[39m gca()\u001b[39m.\u001b[39mimshow(\n\u001b[0;32m   2641\u001b[0m         X, cmap\u001b[39m=\u001b[39mcmap, norm\u001b[39m=\u001b[39mnorm, aspect\u001b[39m=\u001b[39maspect,\n\u001b[0;32m   2642\u001b[0m         interpolation\u001b[39m=\u001b[39minterpolation, alpha\u001b[39m=\u001b[39malpha, vmin\u001b[39m=\u001b[39mvmin,\n\u001b[0;32m   2643\u001b[0m         vmax\u001b[39m=\u001b[39mvmax, origin\u001b[39m=\u001b[39morigin, extent\u001b[39m=\u001b[39mextent,\n\u001b[0;32m   2644\u001b[0m         interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[0;32m   2645\u001b[0m         filternorm\u001b[39m=\u001b[39mfilternorm, filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[0;32m   2646\u001b[0m         url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m: data} \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}),\n\u001b[0;32m   2647\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2648\u001b[0m     sci(__ret)\n\u001b[0;32m   2649\u001b[0m     \u001b[39mreturn\u001b[39;00m __ret\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\matplotlib\\_api\\deprecation.py:456\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m name_idx:\n\u001b[0;32m    451\u001b[0m     warn_deprecated(\n\u001b[0;32m    452\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    455\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\matplotlib\\__init__.py:1412\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1409\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m   1410\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1411\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1412\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39m\u001b[39mmap\u001b[39m(sanitize_sequence, args), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1414\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1415\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[0;32m   1416\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\matplotlib\\axes\\_axes.py:5488\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5481\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m   5482\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap, norm, interpolation,\n\u001b[0;32m   5483\u001b[0m                       origin, extent, filternorm\u001b[39m=\u001b[39mfilternorm,\n\u001b[0;32m   5484\u001b[0m                       filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[0;32m   5485\u001b[0m                       interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[0;32m   5486\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 5488\u001b[0m im\u001b[39m.\u001b[39;49mset_data(X)\n\u001b[0;32m   5489\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5490\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5491\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\matplotlib\\image.py:706\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39msafe_masked_invalid(A, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    704\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39muint8 \u001b[39mand\u001b[39;00m\n\u001b[0;32m    705\u001b[0m         \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mcan_cast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, \u001b[39mfloat\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msame_kind\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[1;32m--> 706\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImage data of dtype \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m cannot be converted to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    707\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype))\n\u001b[0;32m    709\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    710\u001b[0m     \u001b[39m# If just one dimension assume scalar and apply colormap\u001b[39;00m\n\u001b[0;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A[:, :, \u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Viewing the data\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# To view first 6 images\n",
    "for i in range(6):\n",
    "    # plt.subplot(nrows, ncols, index)\n",
    "    ax = plt.subplot(2, 3, i+1)\n",
    "    img_dir = 'F:\\Yash\\Dataset_SSOC\\train_v2\\train'+train.loc[i, 'FILENAME']\n",
    "    # cv2.imread(path=img_dir, flag=specifies the way in which image should be read)\n",
    "    image = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n",
    "    # cv2.imread(X=data of the image, colormap instance)\n",
    "    plt.imshow(image, cmap = 'gray')\n",
    "    plt.title(train.loc[i, 'IDENTITY'], fontsize=12)\n",
    "    plt.axis('off')\n",
    "# Adjusts the subplot layout parameters\n",
    "plt.subplots_adjust(wspace=0.2, hspace=-0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs in train set      :  565\n",
      "Number of NaNs in validation set :  78\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the data by checking for null values\n",
    "print(\"Number of NaNs in train set      : \", train['IDENTITY'].isnull().sum())\n",
    "print(\"Number of NaNs in validation set : \", valid['IDENTITY'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the null values from the training and testing data\n",
    "train.dropna(axis=0, inplace=True)\n",
    "valid.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Image data of dtype object cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\Yash\\SSOC\\project_1\\DL-Simplified\\Handwriting Recognition Project\\Model\\handwriting_recognition_project.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#X66sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m img_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mF:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mYash\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mDataset_SSOC\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mrain_v2\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mrain\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39munreadable\u001b[39m.\u001b[39mloc[i, \u001b[39m'\u001b[39m\u001b[39mFILENAME\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#X66sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(img_dir, cv2\u001b[39m.\u001b[39mIMREAD_GRAYSCALE)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#X66sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m plt\u001b[39m.\u001b[39;49mimshow(image, cmap \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mgray\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#X66sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(unreadable\u001b[39m.\u001b[39mloc[i, \u001b[39m'\u001b[39m\u001b[39mIDENTITY\u001b[39m\u001b[39m'\u001b[39m], fontsize\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#X66sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m plt\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39moff\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\matplotlib\\_api\\deprecation.py:456\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m name_idx:\n\u001b[0;32m    451\u001b[0m     warn_deprecated(\n\u001b[0;32m    452\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    455\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\matplotlib\\pyplot.py:2640\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mimshow)\n\u001b[0;32m   2635\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimshow\u001b[39m(\n\u001b[0;32m   2636\u001b[0m         X, cmap\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, norm\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, aspect\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, interpolation\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2637\u001b[0m         alpha\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, origin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, extent\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m,\n\u001b[0;32m   2638\u001b[0m         interpolation_stage\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, filternorm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, filterrad\u001b[39m=\u001b[39m\u001b[39m4.0\u001b[39m,\n\u001b[0;32m   2639\u001b[0m         resample\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, url\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 2640\u001b[0m     __ret \u001b[39m=\u001b[39m gca()\u001b[39m.\u001b[39mimshow(\n\u001b[0;32m   2641\u001b[0m         X, cmap\u001b[39m=\u001b[39mcmap, norm\u001b[39m=\u001b[39mnorm, aspect\u001b[39m=\u001b[39maspect,\n\u001b[0;32m   2642\u001b[0m         interpolation\u001b[39m=\u001b[39minterpolation, alpha\u001b[39m=\u001b[39malpha, vmin\u001b[39m=\u001b[39mvmin,\n\u001b[0;32m   2643\u001b[0m         vmax\u001b[39m=\u001b[39mvmax, origin\u001b[39m=\u001b[39morigin, extent\u001b[39m=\u001b[39mextent,\n\u001b[0;32m   2644\u001b[0m         interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[0;32m   2645\u001b[0m         filternorm\u001b[39m=\u001b[39mfilternorm, filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[0;32m   2646\u001b[0m         url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m: data} \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}),\n\u001b[0;32m   2647\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2648\u001b[0m     sci(__ret)\n\u001b[0;32m   2649\u001b[0m     \u001b[39mreturn\u001b[39;00m __ret\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\matplotlib\\_api\\deprecation.py:456\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m name_idx:\n\u001b[0;32m    451\u001b[0m     warn_deprecated(\n\u001b[0;32m    452\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    455\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\matplotlib\\__init__.py:1412\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1409\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m   1410\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1411\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1412\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39m\u001b[39mmap\u001b[39m(sanitize_sequence, args), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1414\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1415\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[0;32m   1416\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\matplotlib\\axes\\_axes.py:5488\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5481\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m   5482\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap, norm, interpolation,\n\u001b[0;32m   5483\u001b[0m                       origin, extent, filternorm\u001b[39m=\u001b[39mfilternorm,\n\u001b[0;32m   5484\u001b[0m                       filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[0;32m   5485\u001b[0m                       interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[0;32m   5486\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 5488\u001b[0m im\u001b[39m.\u001b[39;49mset_data(X)\n\u001b[0;32m   5489\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5490\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5491\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\matplotlib\\image.py:706\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39msafe_masked_invalid(A, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    704\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39muint8 \u001b[39mand\u001b[39;00m\n\u001b[0;32m    705\u001b[0m         \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mcan_cast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, \u001b[39mfloat\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msame_kind\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[1;32m--> 706\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImage data of dtype \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m cannot be converted to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    707\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype))\n\u001b[0;32m    709\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    710\u001b[0m     \u001b[39m# If just one dimension assume scalar and apply colormap\u001b[39;00m\n\u001b[0;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A[:, :, \u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Removing the data with the label 'UNREADABLE'\n",
    "unreadable = train[train['IDENTITY'] == 'UNREADABLE']\n",
    "# resets the index of the DataFrame, and uses the default one instead\n",
    "unreadable.reset_index(inplace = True, drop=True)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i in range(6):\n",
    "    ax = plt.subplot(2, 3, i+1)\n",
    "    img_dir = 'F:\\Yash\\Dataset_SSOC\\train_v2\\train'+unreadable.loc[i, 'FILENAME']\n",
    "    image = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n",
    "    plt.imshow(image, cmap = 'gray')\n",
    "    plt.title(unreadable.loc[i, 'IDENTITY'], fontsize=12)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.2, hspace=-0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['IDENTITY'] != 'UNREADABLE']\n",
    "valid = valid[valid['IDENTITY'] != 'UNREADABLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting some lowercase labels to uppercase to maintain uniformity\n",
    "train['IDENTITY'] = train['IDENTITY'].str.upper()\n",
    "valid['IDENTITY'] = valid['IDENTITY'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting the index to maintain uniform leveling\n",
    "train.reset_index(inplace = True, drop=True) \n",
    "valid.reset_index(inplace = True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    (h, w) = img.shape\n",
    "    # blank white image\n",
    "    final_img = np.ones([64, 256])*255 \n",
    "    \n",
    "    # cropping the image if the width and height are greater than \n",
    "    # 256 and 64 respectively\n",
    "    if w > 256:\n",
    "        img = img[:, :256]\n",
    "        \n",
    "    if h > 64:\n",
    "        img = img[:64, :]\n",
    "    \n",
    "    # rotating the image clockwise to bring the image shape to (x,y)\n",
    "    final_img[:h, :w] = img\n",
    "    return cv2.rotate(final_img, cv2.ROTATE_90_CLOCKWISE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model 30000 images and the validating it on 3000 images\n",
    "train_size = 30000\n",
    "valid_size= 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mf:\\Yash\\SSOC\\project_1\\DL-Simplified\\Handwriting Recognition Project\\Model\\handwriting_recognition_project.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y105sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m img_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mF:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mYash\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mDataset_SSOC\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mrain_v2\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mrain\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mtrain\u001b[39m.\u001b[39mloc[i, \u001b[39m'\u001b[39m\u001b[39mFILENAME\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y105sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(img_dir, cv2\u001b[39m.\u001b[39mIMREAD_GRAYSCALE)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y105sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m image \u001b[39m=\u001b[39m preprocess(image)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y105sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m image \u001b[39m=\u001b[39m image\u001b[39m/\u001b[39m\u001b[39m255.\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y105sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m train_x\u001b[39m.\u001b[39mappend(image)\n",
      "\u001b[1;32mf:\\Yash\\SSOC\\project_1\\DL-Simplified\\Handwriting Recognition Project\\Model\\handwriting_recognition_project.ipynb Cell 12\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y105sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess\u001b[39m(img):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y105sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     (h, w) \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mshape\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y105sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     final_img \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones([\u001b[39m64\u001b[39m, \u001b[39m256\u001b[39m])\u001b[39m*\u001b[39m\u001b[39m255\u001b[39m \u001b[39m# blank white image\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y105sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# crop\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# training\n",
    "train_x = []\n",
    "\n",
    "for i in range(train_size):\n",
    "    img_dir = 'F:\\Yash\\Dataset_SSOC\\train_v2\\train'+train.loc[i, 'FILENAME']\n",
    "    image = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n",
    "    image = preprocess(image)\n",
    "    image = image/255.\n",
    "    train_x.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mf:\\Yash\\SSOC\\project_1\\DL-Simplified\\Handwriting Recognition Project\\Model\\handwriting_recognition_project.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y106sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m img_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mF:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mYash\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mDataset_SSOC\u001b[39m\u001b[39m\\v\u001b[39;00m\u001b[39malidation_v2\u001b[39m\u001b[39m\\v\u001b[39;00m\u001b[39malidation\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mvalid\u001b[39m.\u001b[39mloc[i, \u001b[39m'\u001b[39m\u001b[39mFILENAME\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y106sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(img_dir, cv2\u001b[39m.\u001b[39mIMREAD_GRAYSCALE)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y106sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m image \u001b[39m=\u001b[39m preprocess(image)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y106sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m image \u001b[39m=\u001b[39m image\u001b[39m/\u001b[39m\u001b[39m255.\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y106sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m valid_x\u001b[39m.\u001b[39mappend(image)\n",
      "\u001b[1;32mf:\\Yash\\SSOC\\project_1\\DL-Simplified\\Handwriting Recognition Project\\Model\\handwriting_recognition_project.ipynb Cell 13\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y106sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess\u001b[39m(img):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y106sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     (h, w) \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mshape\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y106sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     final_img \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones([\u001b[39m64\u001b[39m, \u001b[39m256\u001b[39m])\u001b[39m*\u001b[39m\u001b[39m255\u001b[39m \u001b[39m# blank white image\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y106sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# crop\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# validating\n",
    "valid_x = []\n",
    "\n",
    "for i in range(valid_size):\n",
    "    img_dir = 'F:\\Yash\\Dataset_SSOC\\validation_v2\\validation'+valid.loc[i, 'FILENAME']\n",
    "    image = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n",
    "    image = preprocess(image)\n",
    "    image = image/255.\n",
    "    valid_x.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .reshape(a: array_like = -1(we want numpy to figure out as the dimensions are unknown))\n",
    "train_x = np.array(train_x).reshape(-1, 256, 64, 1)\n",
    "valid_x = np.array(valid_x).reshape(-1, 256, 64, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels are converted to numbers representing each character\n",
    "# labels are then prepared for Connectionist Temporal Classification Loss (CTC Loss)\n",
    "alphabets = u\"ABCDEFGHIJKLMNOPQRSTUVWXYZ-' \"\n",
    "# max length of input labels\n",
    "max_str_len = 24 \n",
    "# +1 for ctc pseudo blank\n",
    "num_of_characters = len(alphabets) + 1 \n",
    "# max length of predicted labels\n",
    "num_of_timestamps = 64 \n",
    "\n",
    "\n",
    "def label_to_num(label):\n",
    "    label_num = []\n",
    "    for ch in label:\n",
    "        label_num.append(alphabets.find(ch))\n",
    "        \n",
    "    return np.array(label_num)\n",
    "\n",
    "def num_to_label(num):\n",
    "    ret = \"\"\n",
    "    for ch in num:\n",
    "        if ch == -1:  # CTC Blank\n",
    "            break\n",
    "        else:\n",
    "            ret+=alphabets[ch]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JEBASTIN \n",
      " [ 9  4  1  0 18 19  8 13]\n"
     ]
    }
   ],
   "source": [
    "name = 'JEBASTIN'\n",
    "print(name, '\\n',label_to_num(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_y contains the true labels converted to numbers and padded with -1. \n",
    "# The length of each label is equal to max_str_len.\n",
    "train_y = np.ones([train_size, max_str_len]) * -1\n",
    "# train_label_len contains the length of each true label (without padding)\n",
    "train_label_len = np.zeros([train_size, 1])\n",
    "# train_input_len contains the length of each predicted label. \n",
    "# The length of all the predicted labels is constant i.e number of timestamps - 2.\n",
    "train_input_len = np.ones([train_size, 1]) * (num_of_timestamps-2)\n",
    "# train_output is a dummy output for ctc loss.\n",
    "train_output = np.zeros([train_size])\n",
    "\n",
    "for i in range(train_size):\n",
    "    train_label_len[i] = len(train.loc[i, 'IDENTITY'])\n",
    "    train_y[i, 0:len(train.loc[i, 'IDENTITY'])]= label_to_num(train.loc[i, 'IDENTITY'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y = np.ones([valid_size, max_str_len]) * -1\n",
    "valid_label_len = np.zeros([valid_size, 1])\n",
    "valid_input_len = np.ones([valid_size, 1]) * (num_of_timestamps-2)\n",
    "valid_output = np.zeros([valid_size])\n",
    "\n",
    "for i in range(valid_size):\n",
    "    valid_label_len[i] = len(valid.loc[i, 'IDENTITY'])\n",
    "    valid_y[i, 0:len(valid.loc[i, 'IDENTITY'])]= label_to_num(valid.loc[i, 'IDENTITY'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label :  NOUR \n",
      "train_y :  [13. 14. 20. 17. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1.] \n",
      "train_label_len :  [4.] \n",
      "train_input_len :  [62.]\n"
     ]
    }
   ],
   "source": [
    "print('True label : ',train.loc[100, 'IDENTITY'] , '\\ntrain_y : ',train_y[100],'\\ntrain_label_len : ',train_label_len[100], \n",
    "      '\\ntrain_input_len : ', train_input_len[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 256, 64, 1)]      0         \n",
      "                                                                 \n",
      " conv1 (Conv2D)              (None, 256, 64, 32)       320       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 256, 64, 32)      128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 256, 64, 32)       0         \n",
      "                                                                 \n",
      " max1 (MaxPooling2D)         (None, 128, 32, 32)       0         \n",
      "                                                                 \n",
      " conv2 (Conv2D)              (None, 128, 32, 64)       18496     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 128, 32, 64)      256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 128, 32, 64)       0         \n",
      "                                                                 \n",
      " max2 (MaxPooling2D)         (None, 64, 16, 64)        0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64, 16, 64)        0         \n",
      "                                                                 \n",
      " conv3 (Conv2D)              (None, 64, 16, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 64, 16, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 64, 16, 128)       0         \n",
      "                                                                 \n",
      " max3 (MaxPooling2D)         (None, 64, 8, 128)        0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64, 8, 128)        0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 64, 1024)          0         \n",
      "                                                                 \n",
      " dense1 (Dense)              (None, 64, 64)            65600     \n",
      "                                                                 \n",
      " lstm1 (Bidirectional)       (None, 64, 512)           657408    \n",
      "                                                                 \n",
      " lstm2 (Bidirectional)       (None, 64, 512)           1574912   \n",
      "                                                                 \n",
      " dense2 (Dense)              (None, 64, 30)            15390     \n",
      "                                                                 \n",
      " softmax (Activation)        (None, 64, 30)            0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,406,878\n",
      "Trainable params: 2,406,430\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_data = Input(shape=(256, 64, 1), name='input')\n",
    "\n",
    "inner = Conv2D(32, (3, 3), padding='same', name='conv1', kernel_initializer='he_normal')(input_data)  \n",
    "inner = BatchNormalization()(inner)\n",
    "inner = Activation('relu')(inner)\n",
    "inner = MaxPooling2D(pool_size=(2, 2), name='max1')(inner)\n",
    "\n",
    "inner = Conv2D(64, (3, 3), padding='same', name='conv2', kernel_initializer='he_normal')(inner)\n",
    "inner = BatchNormalization()(inner)\n",
    "inner = Activation('relu')(inner)\n",
    "inner = MaxPooling2D(pool_size=(2, 2), name='max2')(inner)\n",
    "inner = Dropout(0.3)(inner)\n",
    "\n",
    "inner = Conv2D(128, (3, 3), padding='same', name='conv3', kernel_initializer='he_normal')(inner)\n",
    "inner = BatchNormalization()(inner)\n",
    "inner = Activation('relu')(inner)\n",
    "inner = MaxPooling2D(pool_size=(1, 2), name='max3')(inner)\n",
    "inner = Dropout(0.3)(inner)\n",
    "\n",
    "# CNN to RNN\n",
    "inner = Reshape(target_shape=((64, 1024)), name='reshape')(inner)\n",
    "inner = Dense(64, activation='relu', kernel_initializer='he_normal', name='dense1')(inner)\n",
    "\n",
    "## RNN\n",
    "inner = Bidirectional(LSTM(256, return_sequences=True), name = 'lstm1')(inner)\n",
    "inner = Bidirectional(LSTM(256, return_sequences=True), name = 'lstm2')(inner)\n",
    "\n",
    "## OUTPUT\n",
    "inner = Dense(num_of_characters, kernel_initializer='he_normal',name='dense2')(inner)\n",
    "y_pred = Activation('softmax', name='softmax')(inner)\n",
    "\n",
    "model = Model(inputs=input_data, outputs=y_pred)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the ctc loss function\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = Input(name='gtruth_labels', shape=[max_str_len], dtype='float32')\n",
    "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "\n",
    "ctc_loss = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n",
    "model_final = Model(inputs=[input_data, labels, input_length, label_length], outputs=ctc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"f:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"f:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"f:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"f:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"f:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"f:\\Anaconda\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 200, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Layer \"model_3\" expects 4 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 24) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 1) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 1) dtype=float32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mf:\\Yash\\SSOC\\project_1\\DL-Simplified\\Handwriting Recognition Project\\Model\\handwriting_recognition_project.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y122sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# the loss calculation occurs elsewhere, so we use a dummy lambda function for the loss\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y122sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model_final\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mctc\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mlambda\u001b[39;00m y_true, y_pred: y_pred}, optimizer\u001b[39m=\u001b[39mAdam(learning_rate \u001b[39m=\u001b[39m \u001b[39m0.0001\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y122sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model_final\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49m[train_x, train_y, train_input_len, train_label_len], y\u001b[39m=\u001b[39;49mtrain_output, \n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y122sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                 validation_data\u001b[39m=\u001b[39;49m([valid_x, valid_y, valid_input_len, valid_label_len], valid_output),\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y122sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                 epochs\u001b[39m=\u001b[39;49m\u001b[39m60\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m)\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileb33w_s9p.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"f:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"f:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"f:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"f:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"f:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"f:\\Anaconda\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 200, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Layer \"model_3\" expects 4 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 24) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 1) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 1) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# the loss calculation occurs elsewhere, so we use a dummy lambda function for the loss\n",
    "model_final.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=Adam(learning_rate = 0.0001))\n",
    "\n",
    "model_final.fit(x=[train_x, train_y, train_input_len, train_label_len], y=train_output, \n",
    "                validation_data=([valid_x, valid_y, valid_input_len, valid_label_len], valid_output),\n",
    "                epochs=60, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pop from an empty set'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mf:\\Yash\\SSOC\\project_1\\DL-Simplified\\Handwriting Recognition Project\\Model\\handwriting_recognition_project.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y123sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m preds \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(valid_x)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y123sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m decoded \u001b[39m=\u001b[39m K\u001b[39m.\u001b[39mget_value(K\u001b[39m.\u001b[39mctc_decode(preds, input_length\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mones(preds\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\u001b[39m*\u001b[39mpreds\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y123sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                    greedy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y123sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m prediction \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\keras\\engine\\data_adapter.py:246\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m (sample_weights, _, _) \u001b[39m=\u001b[39m training_utils\u001b[39m.\u001b[39mhandle_partial_sample_weights(\n\u001b[0;32m    242\u001b[0m     y, sample_weights, sample_weight_modes, check_all_flat\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    244\u001b[0m inputs \u001b[39m=\u001b[39m pack_x_y_sample_weight(x, y, sample_weights)\n\u001b[1;32m--> 246\u001b[0m num_samples \u001b[39m=\u001b[39m \u001b[39mset\u001b[39;49m(\u001b[39mint\u001b[39;49m(i\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mflatten(inputs))\u001b[39m.\u001b[39;49mpop()\n\u001b[0;32m    247\u001b[0m _check_data_cardinality(inputs)\n\u001b[0;32m    249\u001b[0m \u001b[39m# If batch_size is not passed but steps is, calculate from the input data.\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# Default to 32 for backwards compat.\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'pop from an empty set'"
     ]
    }
   ],
   "source": [
    "preds = model.predict(valid_x)\n",
    "decoded = K.get_value(K.ctc_decode(preds, input_length=np.ones(preds.shape[0])*preds.shape[1], \n",
    "                                   greedy=True)[0][0])\n",
    "\n",
    "prediction = []\n",
    "for i in range(valid_size):\n",
    "    prediction.append(num_to_label(decoded[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\Yash\\SSOC\\project_1\\DL-Simplified\\Handwriting Recognition Project\\Model\\handwriting_recognition_project.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y124sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y124sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(valid_size):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y124sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     pr \u001b[39m=\u001b[39m prediction[i]\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y124sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     tr \u001b[39m=\u001b[39m y_true[i]\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y124sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     total_char \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(tr)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prediction' is not defined"
     ]
    }
   ],
   "source": [
    "y_true = valid.loc[0:valid_size, 'IDENTITY']\n",
    "correct_char = 0\n",
    "total_char = 0\n",
    "correct = 0\n",
    "\n",
    "for i in range(valid_size):\n",
    "    pr = prediction[i]\n",
    "    tr = y_true[i]\n",
    "    total_char += len(tr)\n",
    "    \n",
    "    for j in range(min(len(tr), len(pr))):\n",
    "        if tr[j] == pr[j]:\n",
    "            correct_char += 1\n",
    "            \n",
    "    if pr == tr :\n",
    "        correct += 1 \n",
    "    \n",
    "print('Correct characters predicted : %.2f%%' %(correct_char*100/total_char))\n",
    "print('Correct words predicted      : %.2f%%' %(correct*100/valid_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/handwriting-recognition/written_name_test_v2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\Yash\\SSOC\\project_1\\DL-Simplified\\Handwriting Recognition Project\\Model\\handwriting_recognition_project.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y125sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39m/kaggle/input/handwriting-recognition/written_name_test_v2.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y125sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m15\u001b[39m, \u001b[39m10\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Yash/SSOC/project_1/DL-Simplified/Handwriting%20Recognition%20Project/Model/handwriting_recognition_project.ipynb#Y125sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m6\u001b[39m):\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m     f,\n\u001b[0;32m   1219\u001b[0m     mode,\n\u001b[0;32m   1220\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1221\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1222\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1223\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1224\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1225\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1226\u001b[0m )\n\u001b[0;32m   1227\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    790\u001b[0m             handle,\n\u001b[0;32m    791\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    792\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    793\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    794\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    795\u001b[0m         )\n\u001b[0;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/handwriting-recognition/written_name_test_v2.csv'"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('/kaggle/input/handwriting-recognition/written_name_test_v2.csv')\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(6):\n",
    "    ax = plt.subplot(2, 3, i+1)\n",
    "    img_dir = '/kaggle/input/handwriting-recognition/test_v2/test/'+test.loc[i, 'FILENAME']\n",
    "    image = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    \n",
    "    image = preprocess(image)\n",
    "    image = image/255.\n",
    "    pred = model.predict(image.reshape(1, 256, 64, 1))\n",
    "    decoded = K.get_value(K.ctc_decode(pred, input_length=np.ones(pred.shape[0])*pred.shape[1], \n",
    "                                       greedy=True)[0][0])\n",
    "    plt.title(num_to_label(decoded[0]), fontsize=12)\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.subplots_adjust(wspace=0.2, hspace=-0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5d6c2be6262c78dadbf075e7ed7eef3fc1090c1044fdcff36d11e01ba01e819"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
